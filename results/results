one epoch is around 12.55 optimization steps


base has weight decay of 0

base model Relu valid: 83%, test: 71.8%
base model Tanh valid: 79.5%, test: 65.6%

base model Sigmoid valid: 81.1%, test: 66%

Relu AdamW valid: 83%, test: 72%


mlp base
              precision    recall  f1-score   support

           0       0.29      0.08      0.13       649
           1       0.96      0.78      0.86      1988
           2       0.93      0.90      0.91       452
           3       0.78      0.70      0.74       370
           4       0.29      0.66      0.40       726
           5       0.86      0.84      0.85      2399

    accuracy                           0.72      6584
   macro avg       0.68      0.66      0.65      6584
weighted avg       0.77      0.72      0.73      6584


Big model is ReLU with 150 units in the first two layers, using AdamW optimizer with weight decay of .001.


The biggest increases in performance were due to architecture type changes, from MLP to CNN, the former is a naive approach that can model any kind of data and the latter imposes inductive biases such as taking advantage of time shifting invariances


CNN


cnn base valid acc: 91%, test: 85% - 87%

cnn base
              precision    recall  f1-score   support

           0       0.53      0.82      0.64       649
           1       0.97      0.96      0.97      1988
           2       0.86      1.00      0.92       452
           3       1.00      0.80      0.89       370
           4       0.85      0.72      0.78       726
           5       0.93      0.84      0.88      2399

    accuracy                           0.87      6584
   macro avg       0.86      0.86      0.85      6584
weighted avg       0.89      0.87      0.88      6584



cnn leakyReLU valid acc: 92.3%, test: 86.5%  Took longer to train, around double optimization steps to achieve the same performance.
It is worth noting that it behaves better with sub-represented classes (Sitting, Standing and Upstairs). Maybe because gradients are not completely lost for signals that might be weak due to over-represented classes.

              precision    recall  f1-score   support

           0       0.61      0.79      0.69       649
           1       0.91      0.97      0.94      1988
           2       0.93      0.92      0.92       452
           3       1.00      0.89      0.94       370
           4       0.83      0.63      0.72       726
           5       0.89      0.86      0.87      2399

    accuracy                           0.86      6584
   macro avg       0.86      0.84      0.85      6584
weighted avg       0.87      0.86      0.86      6584



cnn maxpool instead of avgpool valid acc: 89.5%, test: 86%. Using only average pooling instead hurts perfomance a small amount. Not significant

              precision    recall  f1-score   support

           0       0.52      0.85      0.64       649
           1       0.98      0.97      0.97      1988
           2       0.79      0.99      0.88       452
           3       0.96      0.66      0.78       370
           4       0.91      0.62      0.74       726
           5       0.91      0.85      0.88      2399

    accuracy                           0.86      6584
   macro avg       0.84      0.82      0.81      6584
weighted avg       0.89      0.86      0.86      6584




The base model size with no dropout has a drop of 2-3 points in performance (91.5%% valid, 84.75% test)

Decreasing feature maps to 60% of their size las a limited impact on performance (89.9% valid, 86.8% test), which doesn't decrease significantly if no dropout is used (89.6% valid, 86% test). However, this smaller model takes more than double optimization steps to train when using dropout.
The smaller effect of dropout in


Reflections

Gaussian noise

takes more optimization steps, more stable learning process (for validation accuracy)


.001
    valid 89.9%
              precision    recall  f1-score   support

           0       0.53      0.70      0.61       649
           1       0.98      0.96      0.97      1988
           2       0.89      0.99      0.94       452
           3       0.98      0.84      0.90       370
           4       0.80      0.72      0.75       726
           5       0.90      0.86      0.88      2399

    accuracy                           0.87      6584
   macro avg       0.85      0.84      0.84      6584
weighted avg       0.88      0.87      0.87      6584

ACCURACY:  0.866494532199271


.005
    valid 90.4%
              precision    recall  f1-score   support

           0       0.74      0.75      0.74       649
           1       0.92      0.96      0.94      1988
           2       0.80      0.99      0.88       452
           3       0.97      0.68      0.80       370
           4       0.87      0.71      0.78       726
           5       0.91      0.92      0.91      2399

    accuracy                           0.88      6584
   macro avg       0.87      0.84      0.84      6584
weighted avg       0.89      0.88      0.88      6584

ACCURACY:  0.883505467800729


.01
    valid 90%
              precision    recall  f1-score   support

           0       0.56      0.82      0.66       649
           1       0.98      0.96      0.97      1988
           2       0.78      0.99      0.87       452
           3       0.97      0.66      0.79       370
           4       0.91      0.69      0.79       726
           5       0.90      0.87      0.89      2399

    accuracy                           0.87      6584
   macro avg       0.85      0.83      0.83      6584
weighted avg       0.89      0.87      0.87      6584

ACCURACY:  0.8686208991494532


.02
    valid 91.1%%
              precision    recall  f1-score   support

           0       0.58      0.67      0.62       649
           1       0.91      0.97      0.94      1988
           2       0.95      1.00      0.97       452
           3       0.99      0.93      0.96       370
           4       0.88      0.71      0.78       726
           5       0.89      0.85      0.87      2399

    accuracy                           0.87      6584
   macro avg       0.87      0.85      0.86      6584
weighted avg       0.87      0.87      0.87      6584

ACCURACY:  0.8681652490886999

.05
    valid 90.3%
              precision    recall  f1-score   support

           0       0.52      0.72      0.61       649
           1       0.99      0.96      0.97      1988
           2       0.82      0.99      0.90       452
           3       0.96      0.73      0.83       370
           4       0.89      0.64      0.74       726
           5       0.85      0.86      0.85      2399

    accuracy                           0.85      6584
   macro avg       0.84      0.82      0.82      6584
weighted avg       0.87      0.85      0.86      6584

ACCURACY:  0.8520656136087484

.1
    valid 89.39%
              precision    recall  f1-score   support

           0       0.69      0.62      0.65       649
           1       0.99      0.96      0.97      1988
           2       0.83      0.98      0.90       452
           3       0.76      0.77      0.77       370
           4       0.81      0.56      0.66       726
           5       0.85      0.95      0.90      2399

    accuracy                           0.87      6584
   macro avg       0.82      0.81      0.81      6584
weighted avg       0.87      0.87      0.86      6584

ACCURACY:  0.8672539489671932


.2
    valid 83.65%
              precision    recall  f1-score   support

           0       0.60      0.21      0.31       649
           1       1.00      0.94      0.97      1988
           2       0.72      0.91      0.81       452
           3       0.35      0.69      0.47       370
           4       0.47      0.43      0.45       726
           5       0.88      0.93      0.90      2399

    accuracy                           0.79      6584
   macro avg       0.67      0.69      0.65      6584
weighted avg       0.80      0.79      0.78      6584

ACCURACY:  0.7910085054678008


Data augmentation

Different data augmentation techniques are tested, most of them with promising results. Even though this should be obvious, perhaps it is worth noting that augmentation is done randomly, only over the training data and differently for each optimization step

prob timeshift. time is shifted evenly across each channel, as their alignment is important. with certain probability, we shift a random number of time steps, from 1 to the length of the sequence, and the end offset segment put at the beginning of the sequence
in theory, due to the translation invariance of CNNs this shouldn't increase performance. However, possibly due to how the dataset is built, i.e. cutting a continuous time series in chunks that can possibly include more than one class for each chunk, this seems to help model performance

{0: 0.8713547995139733,
0.05: 0.9154009720534629,
0.1: 0.9103888213851762,
0.2: 0.8912515188335358,
0.5: 0.8816828675577156,
0.8: 0.8883657351154314,
1: 0.8964155528554071}

test
{0: 0. 899979334573259,
0.05: 0. 8995660260384377,
0.1: 0.9082455052696838,
0.2: 0.9103120479437901,
0.5: 0.9049390369911139,
0.8: 0.907625542467452,
1:0.9012192601777227ÔΩù

other data augmentation techniques
affine transformations:
translation could be seen as time shift, to which a cnn should be invariant
scaling could be relevant, in the amplitude to make the spikes higher and could also help "simulate" different calibrations in sensors, users energy and abruptness in movement. Here random amplitude scaling is implemented, a different scaling factor for each channel
{-1: 0. 9249,
0.5: 0.9229,
0.2: 0.9063,
0.1: 0.9024,
0.1: 0.8977,
0.2: 0.9127,
0.5: 0.9177,
1: 0.9332}

test
-1: 0. 8386,
0.5: 0.8590,
0.2: 0.8734,
0.1: 0.8768,
0.1: 0.8645,
0.2: 0.8576,
0.5: 0.8517,
1: 0.8599}
here, each scaling is a range around 1, for example .2 would scale the time series in a random factor between .8 and 1.2
a large amount of scaling increases validation accuracy, but due to an unknown reason test accuracy seems to be hurt by it,

scaling in the frequency could also be used, to a certain amount, for users with slower/faster movements, the author did not try this.

cutout or masking can be used for segments of the time series, here, with probability p, a random (length and location) segment of each channel (differently) is masked to 0
validation
0.9: 0.9251
0.75: 0.9328
0.5: 0.9332
0.1: 0.9167

test
0.9: 0.8491
0.75: 0. 8467
0.5: 0.8707
0.1: 0.8462
<INCLUDE MASKING>
again, validation performance is increased while test performance is decreased or constant, -beginning to think this is due to a programming error but almost sure it isn't

rotation. As each channel represents a coordinate, rotation could be performed across these dimensions, possibly simulating that the measuring device is rotated (think of a cellphone put downwards). The author didn't implement this technique due to time restrictions

Ad-hoc technique.
Considering that the device could be switched in x-y-z axis, input channels are randomly switched.
valid
{0.1: 0.9010126059103121,
0.25: 0.9129985534201281,
0.5: 0.9053523455259351,
0.9: 0.9142384790245919}

test
{0.1: 0.8725698663426489,
0.25: 0.8666464155528554,
0.5: 0.8636087484811664,
0.9: 0.85343256379100853
didn't see a significant increase in test performance





Baseline comparison
A random baseline would select randomly across classes, it would be accurate with probability 1/6, yielding 16.6% accuracy, independently of class imbalance
Another baseline for a naive predictor could be selecting the class that appears most of the time, selecting class "Walking" would yield around 35% accuracy
These baselines are well below both models tested here.

Overfitting
Maximum accuracy on validation was 91%, dropped and remained around 88.8%, while validation loss kept increasing from a minimum of .34 to 8
test accuracy was still 87%, this tells us that the CNN doesn't overfit, dropout and model architecture could be to blame
<INCLUDE OVERFITTING>



Batch Normalization
Normalizes inputs across a batch with a few learned parameters, mitigating the well known and feared covariate shift
implemented after the second convolution, slightly increases performance and stabilizes training
implemented after the fourth convolutions, slightly increases performance and stabilizes training 87%
implemented after the second and fourth convolutions, increases performance and stabilizes training 90%

Layer Normalization
Normalizes inputs across time features and channels, implemented after convolution layers but before its activations. Makes training more stable and faster as it reaches good performance in less optimization steps
after the second convolution slightly increases generalization performance to 87.5% test
implemented after the fourth convolution, actually hurts performance, possibly due to the change in intensity over features whose differences might be expressive
implemented after the second and fourth convolutions, actually hurts performance, possibly due to the change in intensity over features whose differences might be expressive

Combining layer normalization after the second convolution and batch normalization after the fourth convolution somehow makes training unstable and doesn't increase performance


Optimizers
Choice of learning rate is fundamental in model performance, slight changes can impact significantly and even make the model useless. Changes in optimizer tested here shouldn't impact significantly, but AdamW performs a little better if using weight decay
Adam
Learning Rate .01 doesn't learn, training is useless. If used Batch Normalization, the model learns, a bit noisily but consistently, but hurts generalization performance 83%
Learning Rate .001 baseline model with good performance around 86%
Learning Rate .0001 makes learning smoother reaching its highest performance earlier in training, however, it hurts generalization performance 80% test
Weight decay of .001 slightly increases generalization performance 1%  (86.9%)
AdamW
Is supposed to perform better when using Weight Decay, here it did perform better than plain Adam
Weight decay of .001 slightly increases generalization performance 1-2%  (87.5%)


Model scaling
for the baseline model, the mean optimization step time is 6.3e-4 seconds
decreasing the number of filters to 50 and 80 in the first and final layers, and a dropout of .3 instead of .5 to prevent underfitting

impact in model performance is not significant 85.8% and optimization time is not affected, with a similar mean optimization step time.
This is also affected by other factors such as hardware, library implementations and other processes running in the operating system.

in this smaller model, completely removing dropout slightly increases performance (86.2%) and does not affect training time

Data scaling
for subsampling the time series it is necessary to decrease the kernel sizes of the model accordingly.
subsampling every 2nd sample for a rate of 10Hz slightly decreases performance 84.5% and doesn't have a significant impact on training time
A more agressive subsampling of one every 5 time steps, does affect model performance significantly to a 75% accuracy

Sampling strategies
Randomly selecting each batch with replacement slightly increases performance ~.5% which may also be due to noise
Oversampling sub represented classes achieves competitive overall performance of 87.4% and indeed increases accuracy for under represented classes, specially "standing" and "sitting" achieving the highest f1 score for this classes with 95%
















