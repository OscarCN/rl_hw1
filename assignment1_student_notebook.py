# -*- coding: utf-8 -*-
"""Copy of Assignment1_student_notebook.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-eZdiTpE_AKIqJuoORakifBkH1DICoHC

# Human Activity Recognition(HAR)

- Reference article: https://towardsdatascience.com/human-activity-recognition-har-tutorial-with-keras-and-core-ml-part-1-8c05e365dfa0
- Reference code: https://github.com/ni79ls/har-keras-coreml/blob/master/Human%20Activity%20Recognition%20with%20Keras%20and%20CoreML.ipynb
- Dataset: https://www.cis.fordham.edu/wisdm/dataset.php

---

* Convert and reformat **accelerometer data** into a **time-sliced representation**
* **Visualize** the accelerometer data
* Reshape the **multi-dimensional tabular data**

## Overview

- **accelererometer data** from a smartphone
    - x, y, z axis acceleration
    - timestamp
    - person ID
- **6 acitivites**
    - Walking
    - Jogging
    - Sitting
    - Standing
    - Upstairs
    - DownStairs
- **Model**: trained to classify between each of the 6 activities.
    1. DNN
    2. 1D CNN
"""

# Commented out IPython magic to ensure Python compatibility.
import os
import warnings
warnings.filterwarnings(action='ignore')

from __future__ import print_function
from matplotlib import pyplot as plt
# %matplotlib inline
import numpy as np
import pandas as pd
import seaborn as sns
from scipy import stats
from IPython.display import display, HTML

from sklearn import metrics
from sklearn.metrics import classification_report
from sklearn import preprocessing

# PyTorch Imports
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import TensorDataset, Dataset, DataLoader

# Check if CUDA is available (for GPU usage)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

"""Constants
- TIME_PERIODS: the length of the time segment
- STEP_DISTANCE: the amount of overlap between two consecutive time segments
"""

# Set some standard parameters upfront
pd.options.display.float_format = '{:.1f}'.format
sns.set() # Default seaborn look and feel
plt.style.use('ggplot')

# Class labels
LABELS = ['Downstairs',
          'Jogging',
          'Sitting',
          'Standing',
          'Upstairs',
          'Walking']

TIME_PERIODS = 80  # The number of steps within one time segment

# The steps to take from one segment to the next; if this value is equal to TIME_PERIODS,
# then there is no overlap between the segments
STEP_DISTANCE = 40

"""# Data

I used `/kaggle/input/activitydetectionimusensor/WISDM_ar_v1.1.1_raw.txt` data.
> (another dataset (later): `/kaggle/input/human-activity-recognition/time_series_data_human_activities.csv`)
"""

# Define some functions to read the data and show some basic info about the data

def read_data(file_path):
    column_names = ['user', 'activity', 'timestamp', 'x-axis', 'y-axis', 'z-axis']
    df = pd.read_csv(file_path, header=None, names=column_names)
    # Last column has a ";" character which must be removed
    df['z-axis'].replace(regex=True,
                         inplace=True,
                         to_replace=r';',
                         value=r'')
    # Transform 'z-axis' column to float
    df['z-axis'] = df['z-axis'].apply(convert_to_float)

    df.dropna(axis=0, how='any', inplace=True)  # Drop NaN values

    return df

def convert_to_float(x):
    try:
        #return np.float(x)
        return float(x)
    except:
        return np.nan

def show_basic_dataframe_info(dataframe):
    # Shape: #_rows, #_columns
    print("Number of rows in the dataframe: %i" % (dataframe.shape[0]))
    print("Number of columns in the dataframe: %i" % (dataframe.shape[1]))

"""The data can be downloaded from **[here](https://www.kaggle.com/datasets/sosoyeong/wisdm-raw)**.

Then 'file_path' the directory to where you have put the data.
"""

#from google.colab import drive
#file_path = '/content/drive/My Drive/Colab Notebooks/IFT6135/WISDM_ar_v1.1_raw.txt'
#drive.mount('/content/drive')

from io import StringIO



file_path = '/Users/oscarcuellar/ocn/mila/rl/hw1/data/WISDM_ar_v1.1/WISDM_ar_v1.1_raw.txt'
file_path = '/Users/oscarcuellar/ocn/mila/rl/hw1/data/WISDM_ar_v1.1_raw.txt'

f = open(file_path, 'r')
csvStringIO = StringIO(f.read().replace(';', ''))
f.close()

column_names = ['user', 'activity', 'timestamp', 'x-axis', 'y-axis', 'z-axis']
df = pd.read_csv(csvStringIO)
df = read_data(file_path)

show_basic_dataframe_info(df)
df.head(10)

# by activity type
df['activity'].value_counts().plot(kind='bar',
                                   title='Training Examples by Activity Type')
plt.show()

# by user
df['user'].value_counts().plot(kind='bar',
                                  title='Training Examples by User')
plt.show()

"""- We have more data for walking and jogging activities more than other activities.
- 36 participants

<br>

**Accelerometer data** for six activities
- sampling rates 20Hz (20 values per second)
    - first 180 records == 9 second interval
        - 1/200 * 180 = 9 seconds
"""

def plot_activity(activity, data):
    fig, (ax0, ax1, ax2) = plt.subplots(nrows=3, figsize=(15, 10), sharex=True)
    plot_axis(ax0, data['timestamp'], data['x-axis'], 'X-Axis')  # x
    plot_axis(ax1, data['timestamp'], data['y-axis'], 'Y-Axis')  # y
    plot_axis(ax2, data['timestamp'], data['z-axis'], 'Z-Axis')  # z
    plt.subplots_adjust(hspace=0.2)
    fig.suptitle(activity)
    plt.subplots_adjust(top=0.90)
    plt.show()


def plot_axis(ax, x, y, title):
    ax.plot(x, y, 'r')
    ax.set_title(title, fontsize=10)
    ax.xaxis.set_visible(False)

    ax.set_ylim([min(y) - np.std(y), max(y) + np.std(y)])
    ax.set_xlim([min(x), max(x)])
    ax.grid(True)


for activity in np.unique(df['activity']):
    subset = df[df['activity'] == activity][:180]  # check only for first 180 records (9 seconds)
    plot_activity(activity, subset)

"""Activies like Walking and Jogging has higher acceleration compared to Sitting.

Add **new column** called **'ActivityEncoded'** to the dataframe
- encoded value for each activity
- Why?
    - DNN only work with numerical labels
        - **need to change string label to number!**
            0. Downstairs
            1. Jogging
            2. Sitting
            3. Standing
            4. Upstairs
            5. Walking
"""

LABEL = 'ActivityEncoded'

le = preprocessing.LabelEncoder()  # string to Integer
df[LABEL] = le.fit_transform(df['activity'].values.ravel())

df.head()

# Split data into train and test set
# train: user 1 ~ 28
# test: user 28 ~

df_train = df[df['user'] <= 28]
df_test = df[df['user'] > 28]

# normalize train data (value range: 0 ~ 1)
# normalization should be applied to test data in the same way
pd.options.mode.chained_assignment = None  # defual='warm'

df_train['x-axis'] = df_train['x-axis'] / df_train['x-axis'].max()
df_train['y-axis'] = df_train['y-axis'] / df_train['y-axis'].max()
df_train['z-axis'] = df_train['z-axis'] / df_train['z-axis'].max()


# round numbers
df_train = df_train.round({'x-axis':4, 'y-axis':4, 'z-axis': 4})
df_train.head()

df_test['x-axis'] = df_test['x-axis'] / df_test['x-axis'].max()
df_test['y-axis'] = df_test['y-axis'] / df_test['y-axis'].max()
df_test['z-axis'] = df_test['z-axis'] / df_test['z-axis'].max()

df_test = df_test.round({'x-axis':4, 'y-axis':4, 'z-axis': 4})


"""Still the dataframe is not ready yet to be fed into a neural network.

So, we need to reshpae it.
"""

# 80 steps => 4 sec (0.05 * 80 = 4)

def create_segments_and_labels(df, time_steps, step, label_name):
    # x, y, z acceleration as features
    N_FEATURES = 3

    # Number of steps to advance in each iteration
    # step = time_steps # no overlap between segments

    segments = []
    labels = []
    for i in range(0, len(df) - time_steps, step):
        xs = df['x-axis'].values[i: i + time_steps]
        ys = df['y-axis'].values[i: i + time_steps]
        zs = df['z-axis'].values[i: i + time_steps]

        # find the most often used label in this segment
        label_mode_result = stats.mode(df[label_name][i: i + time_steps])
        if np.isscalar(label_mode_result.mode):
            label = label_mode_result.mode
        else:
            label = label_mode_result.mode[0]

        segments.append([xs, ys, zs])
        labels.append(label)

    # bring the segments into a better shape
    reshaped_segments = np.asarray(segments, dtype=np.float32).reshape(-1, time_steps, N_FEATURES)
    labels = np.asarray(labels)

    return reshaped_segments, labels  # x, y


x_train, y_train = create_segments_and_labels(df_train,
                                              TIME_PERIODS,
                                              STEP_DISTANCE,
                                              LABEL)  # LABEL = 'ActivityEncoded'

x_test, y_test = create_segments_and_labels(df_test,
                                            TIME_PERIODS,
                                            STEP_DISTANCE,
                                            LABEL)

print(TIME_PERIODS, STEP_DISTANCE)

print('x_train shape: ', x_train.shape)
print(x_train.shape[0], 'training samples')
print('y_train shape: ', y_train.shape)

"""`x_train` has 20868 records of 2D-matrix of shape 80x3.

**Dimensions we need to remeber**

- #_time periods: the number of time periods within 1 record
    - 4 second interval => 80
- #_sensors: 3 (x, y, z axis acceleration)
- #_classes: the number of the nodes for output layer -> 6
"""

# input, output dimensions
n_time_periods, n_sensors = x_train.shape[1], x_train.shape[2]
n_classes = le.classes_.size
print(list(le.classes_))
print(n_time_periods, n_sensors)

"""The input data is 2D (80x3).

"""

input_shape = (n_time_periods * n_sensors)
#x_train = x_train.reshape(x_train.shape[0], input_shape)

print('x_train shape:', x_train.shape)
print('input shape:', input_shape)

x_train = x_train.astype('float32')
y_train = y_train.astype('float32')

def to_categorical(y, num_classes):
    return torch.eye(num_classes)[y]

n_classes = len(np.unique(y_train))  # Assuming y_train is available and contains your class labels
y_train_hot = to_categorical(y_train, n_classes)

print('New y_train shape: ', y_train_hot.shape)

"""In PyTorch, we need to wrap these NumPy arrays into a dataset and then create a DataLoader for batch processing."""

# Convert your numpy arrays to PyTorch tensors
x_train_tensor = torch.tensor(x_train, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train, dtype=torch.long)  # long for CrossEntropyLoss
x_test_tensor = torch.tensor(x_test, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test, dtype=torch.long)

# Create TensorDatasets
train_dataset = TensorDataset(x_train_tensor, y_train_tensor)
test_dataset = TensorDataset(x_test_tensor, y_test_tensor)

# Create DataLoaders
batch_size = 64  # You can change this value as per your need
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

"""# Multi-layer Perceptron"""

class mlp(nn.Module):
    def __init__(self, time_periods, n_classes, **kwargs):
        super(mlp, self).__init__()
        self.time_periods = time_periods
        self.n_classes = n_classes

        self.dim_in = time_periods * 3
        dim_out = n_classes

        self.n_layers = kwargs.get('n_layers', self.dim_in)

        self.layers = nn.ModuleList()
        for l in range(1, 1 + self.n_layers):
            if l == 1:
                n_in = self.dim_in
                n_out = kwargs.get('hid_dim_1', self.dim_in)
            else:
                n_in = kwargs.get('hid_dim_' + str(l-1), self.dim_in)
                n_out = kwargs.get('hid_dim_' + str(l), self.dim_in)

            self.layers.append(nn.Linear(in_features=n_in, out_features=n_out))

        self.lin_out = nn.Linear(self.layers[-1].out_features, out_features=dim_out)

    def forward(self, x):
        _x = x.reshape(x.shape[0], input_shape)
        for layer in self.layers:
            _x = nn.functional.relu(layer(_x))

        return self.lin_out(_x)


def train(model, device, optimizer, steps):
    # Initialize lists to store losses and accuracies
    model.train()

    losses = []
    accuracies = []

    c_steps = 0
    for batch_idx, (inputs, target) in enumerate(train_loader):
        inputs, target = inputs.to(device), target.to(device)

        optimizer.zero_grad()
        output = model(inputs)
        loss = criterion(output, target)

        loss.backward()
        optimizer.step()

        accuracies.append((output.argmax(-1) == target).to(torch.float).mean().item())
        losses.append(loss.item())
        c_steps += 1
        if c_steps >= steps:
            break

    print(f"Train - Loss: {sum(losses)/len(losses):.5f}, Accuracy: {sum(accuracies)/len(accuracies):.5f}")

    return losses, accuracies


def validate(model, device):
    model.train(False)

    # we initialize our kpis to keep track of across batches
    avg_loss = 0.0
    avg_accuracy = 0.0

    with torch.no_grad():  # save some computations
        for inputs, target in test_loader:
            inputs, target = inputs.to(device), target.to(device)
            output = model(inputs)
            loss = criterion(output, target)

            # TODO: code the evaluation loop
            avg_accuracy += (output.argmax(-1) == target).to(torch.float).mean().item()
            avg_loss += loss.item()

    avg_loss /= len(test_loader)
    avg_accuracy /= len(test_loader)

    print(f"Loss: {avg_loss:.5f}, Accuracy: {avg_accuracy:.5f}")

    return avg_loss, avg_accuracy



# Assuming TIME_PERIODS and n_classes are defined
model_mlp = mlp(time_periods=TIME_PERIODS, n_classes=n_classes, n_layers=2, hid_dim_1=256, hid_dim_2=64, hid_dim_3=60)

model_mlp.to(device)

# Print model summary
print(model_mlp)

# Use Pytorch's cross entropy Loss function for a classification task
criterion = nn.CrossEntropyLoss()

# Choose your Optimizer
my_optimizer = torch.optim.AdamW(params=model_mlp.parameters(), lr=.001, weight_decay=.01)

BATCH_SIZE = 400
EPOCHS = 500

train_losses = []
val_losses = []
train_accs = []
val_accs = []

steps = int(len(train_loader) / 1)  # To validate 3 times per epoch

for epoch in range(EPOCHS):

    validate(model_mlp, device)
    tl, ta = train(model_mlp, device, my_optimizer, steps)
    train_losses += tl
    train_accs += ta


# Losses and accuracy plots
def plot_perfomance(train_losses, val_losses):
    raise NotImplementedError

"""Result from the article

![Expectation](attachment:16797bb4-c2ae-4f1b-8a7a-e195e39da9c3.png)

## Test
"""

# normalize test data

'''
df_test['x-axis'] = df_test['x-axis'] / df_test['x-axis'].max()
df_test['y-axis'] = df_test['y-axis'] / df_test['y-axis'].max()
df_test['z-axis'] = df_test['z-axis'] / df_test['z-axis'].max()

df_test = df_test.round({'x-axis':4, 'y-axis':4, 'z-axis': 4})

x_test, y_test = create_segments_and_labels(df_test,
                                            TIME_PERIODS,
                                            STEP_DISTANCE,
                                            LABEL)

x_test = x_test.reshape(x_test.shape[0], input_shape)

x_test = x_test.astype('float32')
y_test = y_test.astype('float32')

y_test = to_categorical(y_train, n_classes)
'''
print('Accuracy on test data: ', val_accs[-1])
print('Loss on test data: ', val_losses[-1])

"""The test accuray is **about 75%**."""

def show_confusion_matrix(validaitons, predictions, title=None):
    matrix = metrics.confusion_matrix(validaitons, predictions)

    plt.figure(figsize=(6, 4))
    sns.heatmap(matrix,
                cmap='coolwarm',
                linecolor='white',
                linewidths=1,
                xticklabels=LABELS,
                yticklabels=LABELS,
                annot=True,
                fmt='d')
    if title: plt.title(title)
    else: plt.title('Confusion Matrix')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.show()

#y_pred_test = model_m.predict(x_test)
#max_y_pred_test = np.argmax(y_pred_test, axis=1)
#max_y_test = np.argmax(y_test, axis=1)

#show_confusion_matrix(max_y_test, max_y_pred_test)

#print(classification_report(max_y_test, max_y_pred_test))

"""---

# 1D CNN

* Reference article: https://blog.goodaudience.com/introduction-to-1d-convolutional-neural-networks-in-keras-for-time-sequences-3a7ff801a2cf
* Reference code: https://github.com/ni79ls/har-keras-cnn/blob/master/20180903_Keras_HAR_WISDM_CNN_v1.0_for_medium.py

 A 1D CNN is very effective when you expect to derive interesting features from shorter (fixed-length) segments of the overall data set and where the location of the feature within the segment is not of high relevance.


This applies well to the analysis of time sequences of sensor data (such as gyroscope or accelerometer data). It also applies to the analysis of any kind of signal data over a fixed-length period (such as audio signals).
"""


class cnn(nn.Module):
    def __init__(self, time_periods, n_sensors, n_classes, **kwargs):
        super(cnn, self).__init__()
        self.time_periods = time_periods
        self.n_sensors = n_sensors
        self.n_classes = n_classes

        # Convolutional layers
        self.conv1 = nn.Conv1d(in_channels=3, out_channels=100, kernel_size=10)
        self.conv2 = nn.Conv1d(in_channels=100, out_channels=100, kernel_size=10)
        self.conv3 = nn.Conv1d(in_channels=100, out_channels=160, kernel_size=10)
        self.conv4 = nn.Conv1d(in_channels=160, out_channels=160, kernel_size=10)

        # Pooling and dropout
        self.maxpool1 = nn.MaxPool1d(kernel_size=3)
        self.avgpool1 = nn.AvgPool1d(kernel_size=2)
        self.dropout = nn.Dropout(.5)
        # Adaptive pool layer to adjust the size before sending to fully connected layer

        # Fully connected layer
        self.out = nn.Linear(in_features=160, out_features=6)

    def forward(self, x):
        # Reshape the input to (batch_size, n_sensors, time_periods)
        #_x = x.reshape...
        _x = x.permute(0, 2, 1)
        # Convolutional layers with ReLU activations
        _x = nn.functional.relu(self.conv1(_x))
        _x = nn.functional.relu(self.conv2(_x))
        _x = self.maxpool1(_x)
        _x = nn.functional.relu(self.conv3(_x))
        _x = nn.functional.relu(self.conv4(_x))

        # Global average pooling and dropout
        _x = self.avgpool1(_x)

        # Flatten the tensor for the fully connected layer
        _x = _x.flatten(start_dim=1)
        _x = self.dropout(_x)
        # Output layer with softmax activation
        _x = self.out(_x)

        #pred = nn.functional.log_softmax(_x, 1)

        # output the loss, Use log_softmax for numerical stability
        return _x


def train_cnn(model, device, optimizer, steps, loss_f):
    # Initialize lists to store losses and accuracies

    model.train()

    losses = []
    accuracies = []

    c_steps = 0
    for batch_idx, (inputs, target) in enumerate(train_loader):
        inputs, target = inputs.to(device), target.to(device)

        optimizer.zero_grad()
        output = model(inputs)
        loss = loss_f(output, target)

        loss.backward()
        optimizer.step()

        accuracies.append((output.argmax(-1) == target).to(torch.float).mean().item())
        losses.append(loss.item())
        c_steps += 1
        if c_steps >= steps:
            break

    print(f"Train - Loss: {sum(losses) / len(losses):.5f}, Accuracy: {sum(accuracies) / len(accuracies):.5f}")

    return losses, accuracies


def validate_cnn(model, device, loss_f):
    model.train(False)

    # we initialize our kpis to keep track of across batches
    avg_loss = 0.0
    avg_accuracy = 0.0

    with torch.no_grad():  # save some computations
        for inputs, target in test_loader:
            inputs, target = inputs.to(device), target.to(device)
            output = model(inputs)
            loss = loss_f(output, target)

            # TODO: code the evaluation loop
            avg_accuracy += (output.argmax(-1) == target).to(torch.float).mean().item()
            avg_loss += loss.item()

    avg_loss /= len(test_loader)
    avg_accuracy /= len(test_loader)

    print(f"Loss: {avg_loss:.5f}, Accuracy: {avg_accuracy:.5f}")

    return avg_loss, avg_accuracy

'''
total = len(test_loader.dataset)
val_acc = 100 * correct / total
val_losses.append(running_loss / len(test_loader))
val_accs.append(val_acc)
'''

# Assuming TIME_PERIODS, n_sensors, and n_classes are defined
model_cnn = cnn(TIME_PERIODS, n_sensors, n_classes)

# Move the model to the device (CPU or GPU)
model_cnn.to(device)

# Print model summary
print(model_cnn)

ce = nn.CrossEntropyLoss()

# Choose your Optimizer
cnn_optimizer = torch.optim.AdamW(params=model_cnn.parameters(), lr=.001, weight_decay=.005)
best_val_loss = float('inf')
patience = 100
trigger_times = 0


cnn_EPOCHS = 500

cnn_train_losses = []
cnn_val_losses = []
cnn_train_accs = []
cnn_val_accs = []

steps = int(len(train_loader) / 2)  # To validate 3 times per epoch

for epoch in range(EPOCHS):

    vl, va = validate_cnn(model_cnn, device, ce)
    tl, ta = train_cnn(model_cnn, device, cnn_optimizer, steps, ce)

    cnn_train_losses += tl
    cnn_train_accs += ta

    cnn_val_losses.append(vl)
    cnn_val_accs.append(va)


print('Training the model...')

train(model_cnn, device, optimizer = optim)
validate_cnn(model_cnn, device)

plot_perfomance(cnn_train_losses, cnn_val_losses)










